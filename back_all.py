import os
import uuid
import pandas as pd
import requests
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import traceback
import io
import json
import logging

# =============================================================================
# ğŸ¯ å¯é…ç½®å‚æ•°åŒºåŸŸ - æ‰€æœ‰é‡è¦å‚æ•°é›†ä¸­åœ¨è¿™é‡Œ
# =============================================================================

# --- Flask æœåŠ¡é…ç½® ---
FLASK_HOST = '0.0.0.0'                    # FlaskæœåŠ¡ç›‘å¬åœ°å€
FLASK_PORT = 8520                          # FlaskæœåŠ¡ç«¯å£
FLASK_DEBUG = True                         # æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼

# --- æ–‡ä»¶URLé…ç½® ---
FILE_SERVER_HOST = '192.168.131.59'     # æ–‡ä»¶æœåŠ¡å™¨åœ°å€
FILE_SERVER_PORT = '8520'                # æ–‡ä»¶æœåŠ¡å™¨ç«¯å£

# --- æ–‡ä»¶è·¯å¾„é…ç½® ---
UPLOAD_FOLDER = 'temp'                     # ä¸Šä¼ æ–‡ä»¶ä¸´æ—¶å­˜å‚¨ç›®å½•
DOWNLOAD_FOLDER = os.path.join('static', 'downloads')  # ä¸‹è½½æ–‡ä»¶å­˜å‚¨ç›®å½•
ALLOWED_EXTENSIONS = {'xlsx', 'xls'}     # å…è®¸çš„æ–‡ä»¶æ‰©å±•å

# --- Dify API é…ç½® ---
DIFY_API_BASE_URL = 'http://192.168.125.223/v1'      # Dify APIåŸºç¡€URL
DIFY_API_KEY = 'Bearer app-6h0jlXree8oBQ10Yyjyk3eCk'  # Dify APIè®¤è¯å¯†é’¥
DIFY_INPUT_VARIABLE_NAME = 'uploaded_file'             # å·¥ä½œæµè¾“å…¥å˜é‡å
DIFY_OUTPUT_VARIABLE_NAME = 'download_link'            # å·¥ä½œæµè¾“å‡ºå˜é‡å

# --- å·¥ä½œæµå¤„ç†é…ç½® ---
DEFAULT_CHUNK_SIZE = 30                   # é»˜è®¤æ¯ä¸ªchunkçš„è¡Œæ•°ï¼ˆå¢åŠ chunkå¤§å°å‡å°‘ä»»åŠ¡æ•°é‡ï¼‰
MAX_WORKERS = 6                          # çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆæé«˜å¹¶å‘åº¦ï¼‰
MAX_RETRIES = -1                          # å•ä¸ªchunkæœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆæ— é™é‡è¯•ï¼Œç¡®ä¿æ¯ä¸ªchunkéƒ½è¢«åˆ†æï¼‰
RETRY_DELAY = 1                           # é‡è¯•é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 180                     # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
FILE_DOWNLOAD_TIMEOUT = 60              # æ–‡ä»¶ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# --- åˆ—åé…ç½® ---
ID_COLUMN_NAME = 'id'                      # é»˜è®¤IDåˆ—å
POSSIBLE_ID_COLUMNS = ['id', 'ID', 'ç¼–å·', 'åºå·']  # å¯èƒ½çš„IDåˆ—ååˆ—è¡¨
COLUMNS_TO_REMOVE = ['id', 'ID']  # æœ€ç»ˆéœ€è¦åˆ é™¤çš„IDåˆ—å

# --- è°ƒè¯•é…ç½® ---
ENABLE_DEBUG_PRINT = True               # æ˜¯å¦å¯ç”¨è°ƒè¯•æ‰“å°
MAX_DEBUG_OUTPUT_LENGTH = 500             # è°ƒè¯•è¾“å‡ºçš„æœ€å¤§é•¿åº¦

# --- é”™è¯¯å¤„ç†é…ç½® ---
ENABLE_TRACEBACK_PRINT = True              # æ˜¯å¦æ‰“å°é”™è¯¯å †æ ˆä¿¡æ¯

# --- æ–‡ä»¶å¤„ç†é…ç½® ---
GENERATE_UNIQUE_FILENAMES = True          # æ˜¯å¦ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼ˆé˜²æ­¢å†²çªï¼‰
UUID_LENGTH = 8                            # å”¯ä¸€æ–‡ä»¶åä¸­UUIDçš„é•¿åº¦

# =============================================================================
# âš™ï¸ è‡ªåŠ¨ç”Ÿæˆçš„URLé…ç½®ï¼ˆé€šå¸¸ä¸éœ€è¦ä¿®æ”¹ï¼‰
# =============================================================================
DIFY_FILE_UPLOAD_URL = f"{DIFY_API_BASE_URL}/files/upload"
DIFY_WORKFLOW_RUN_URL = f"{DIFY_API_BASE_URL}/workflows/run"

# =============================================================================
# ğŸš€ åˆå§‹åŒ– Flask åº”ç”¨
# =============================================================================
app = Flask(__name__)
# é…ç½®CORSä»¥å…è®¸å‰ç«¯è®¿é—®
CORS(app, origins=['http://192.168.131.59:8523', 'http://localhost:8523'], 
     methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
     allow_headers=['Content-Type', 'Authorization'])

# ç¡®ä¿å¿…è¦çš„æ–‡ä»¶å¤¹å­˜åœ¨
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)
if not os.path.exists(DOWNLOAD_FOLDER):
    os.makedirs(DOWNLOAD_FOLDER)

# =============================================================================
# ğŸ“‹ é…ç½®ä¿¡æ¯æ‰“å°
# =============================================================================
if ENABLE_DEBUG_PRINT:
    print(f"=== ğŸ¯ Dify é…ç½®ä¿¡æ¯ ===")
    print(f"è¾“å…¥å˜é‡å: '{DIFY_INPUT_VARIABLE_NAME}'")
    print(f"è¾“å‡ºå˜é‡å: '{DIFY_OUTPUT_VARIABLE_NAME}'")
    print(f"APIç«¯ç‚¹: {DIFY_API_BASE_URL} (æœ¬åœ°éƒ¨ç½²)")
    print(f"è®¤è¯æ–¹å¼: {DIFY_API_KEY[:20]}...")  # æ˜¾ç¤ºè®¤è¯å‰ç¼€å’Œå¯†é’¥éƒ¨åˆ†
    print(f"å·¥ä½œçº¿ç¨‹æ•°: {MAX_WORKERS}")
    print(f"æœ€å¤§é‡è¯•æ¬¡æ•°: {'æ— é™é‡è¯•' if MAX_RETRIES == -1 else MAX_RETRIES}")
    print(f"é»˜è®¤Chunkå¤§å°: {DEFAULT_CHUNK_SIZE}")
    print(f"æ–‡ä»¶æœåŠ¡å™¨: {FILE_SERVER_HOST}:{FILE_SERVER_PORT}")
    print(f"====================")




def process_streaming_response(response):
    """
    å¤„ç†Difyçš„streamingå“åº”ï¼Œè·å–å·¥ä½œæµçš„æœ€ç»ˆè¾“å‡º
    """
    if response.status_code != 200:
        # ä¿å­˜å“åº”æ–‡æœ¬ç”¨äºé”™è¯¯ä¿¡æ¯
        error_text = response.text if hasattr(response, 'text') else "æ— æ³•è·å–é”™è¯¯ä¿¡æ¯"
        raise ValueError(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {error_text}")
    
    full_response = ""
    all_events = []  # æ”¶é›†æ‰€æœ‰äº‹ä»¶
    
    try:
        # æ”¶é›†æ‰€æœ‰å“åº”è¡Œ
        for line in response.iter_lines():
            if not line:
                continue
                
            if not line.startswith(b'data:'):
                continue
                
            data_content = line.decode('utf-8')[5:].strip()
            if data_content == '[DONE]':
                break
                
            try:
                message = json.loads(data_content)
                all_events.append(message)
                
                # æŸ¥æ‰¾å·¥ä½œæµå®Œæˆäº‹ä»¶
                if message.get('event') == 'workflow_finished':
                    # å·¥ä½œæµå®Œæˆäº‹ä»¶åŒ…å«æœ€ç»ˆè¾“å‡º
                    if 'data' in message and 'outputs' in message['data']:
                        full_response = json.dumps(message['data'])
                        break
                elif message.get('event') == 'node_finished':
                    # èŠ‚ç‚¹å®Œæˆäº‹ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸèŠ‚ç‚¹
                    node_data = message.get('data', {})
                    if node_data.get('node_type') == 'end' and 'outputs' in node_data:
                        full_response = json.dumps(node_data)
                        break
                elif 'data' in message and 'outputs' in message['data']:
                    # é€šç”¨æ•°æ®è¾“å‡º
                    full_response = json.dumps(message['data'])
                    
            except json.JSONDecodeError:
                logging.error(f"JSONè§£æé”™è¯¯: {data_content}")
                continue
                
    except Exception as e:
        logging.error(f"å¤„ç†streamingå“åº”æ—¶å‡ºé”™: {e}")
        
    finally:
        response.close()
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„å·¥ä½œæµå®Œæˆäº‹ä»¶ï¼Œè¿”å›æœ€åä¸€ä¸ªåŒ…å«æ•°æ®çš„äº‹ä»¶
    if not full_response and all_events:
        for event in reversed(all_events):
            if 'data' in event and 'outputs' in event['data']:
                full_response = json.dumps(event['data'])
                break
    
    return full_response




# 3. å¹¶è¡Œä»»åŠ¡å•å…ƒå‡½æ•°
def call_small_workflow(chunk_id, df_chunk, which_aspects_value=None):
    print(f"å¼€å§‹å¤„ç† Chunk #{chunk_id}...")
    
    try:
        # --- ç¬¬ä¸€æ­¥: ä¿å­˜åˆ‡åˆ†æ–‡ä»¶åˆ°å¯è®¿é—®ä½ç½®å¹¶ç”ŸæˆURL ---
        print(f"æ­£åœ¨ä¸º Chunk #{chunk_id} ä¿å­˜æ–‡ä»¶å¹¶ç”Ÿæˆè®¿é—®URL...")
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        unique_filename = f"chunk_{chunk_id}_{uuid.uuid4().hex[:UUID_LENGTH]}.xlsx"
        file_path = os.path.join(DOWNLOAD_FOLDER, unique_filename)
        
        # ä¿å­˜Excelæ–‡ä»¶
        df_chunk.to_excel(file_path, index=False)
        
        # ç”Ÿæˆæ–‡ä»¶è®¿é—®URL (ä½¿ç”¨å½“å‰æœåŠ¡çš„ç«¯å£)
        file_url = f"http://{FILE_SERVER_HOST}:{FILE_SERVER_PORT}/downloads/{unique_filename}"
        print(f"Chunk #{chunk_id} æ–‡ä»¶å·²ä¿å­˜ï¼Œè®¿é—®URL: {file_url}")

        # --- ç¬¬äºŒæ­¥: è¿è¡Œå·¥ä½œæµ (ä½¿ç”¨æ–‡ä»¶URLä½œä¸ºè¾“å…¥) ---
        # ä½¿ç”¨ä¼ å…¥çš„which_aspects_valueï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if which_aspects_value is None:
            which_aspects_value = "æ°´è´¨ã€æ°´åŠ¡ã€æ°´åˆ©çš„æ‹›æ ‡ä¿¡æ¯æ•°æ®" # ç¡¬ç¼–ç æ¢å¤
        
        print(f"ä½¿ç”¨which_aspectså€¼: {which_aspects_value}")

        payload = {
            "inputs": {
                DIFY_INPUT_VARIABLE_NAME: {
                    "type": "document",
                    "transfer_method": "remote_url",
                    "url": file_url
                },
                "which_aspects": which_aspects_value  # ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²å€¼ï¼Œä¸éœ€è¦åŒ…è£…æˆå¯¹è±¡
            },
            "response_mode": "streaming",  # å‚è€ƒ func.pyï¼Œä½¿ç”¨ streaming æ¨¡å¼
            "user": 'backend_service_user'
        }
        headers_run = {'Authorization': DIFY_API_KEY, 'Content-Type': 'application/json'}

        print(f"æ­£åœ¨ä¸º Chunk #{chunk_id} è¿è¡Œå·¥ä½œæµ...")
        print(f"å·¥ä½œæµè¯·æ±‚payload: {json.dumps(payload, ensure_ascii=False)}")
        run_response = requests.post(DIFY_WORKFLOW_RUN_URL, headers=headers_run, json=payload, timeout=REQUEST_TIMEOUT, stream=True)
        
        # æ‰“å°å“åº”çŠ¶æ€ç å’Œå¤´ä¿¡æ¯ç”¨äºè°ƒè¯•
        print(f"å·¥ä½œæµå“åº”çŠ¶æ€ç : {run_response.status_code}")
        print(f"å·¥ä½œæµå“åº”å¤´: {dict(run_response.headers)}")
        
        # ç‰¹æ®Šå¤„ç†400é”™è¯¯
        if run_response.status_code == 400:
            error_msg = f"Chunk #{chunk_id} å·¥ä½œæµè¯·æ±‚400é”™è¯¯: {run_response.text}"
            print(error_msg)
            return {'chunk_id': chunk_id, 'status': 'FAILED', 'error': error_msg}
        
        run_response.raise_for_status() 
        
        # å¤„ç†streamingå“åº”ï¼Œå‚è€ƒfunc.pyçš„å®ç°
        streaming_result = process_streaming_response(run_response)
        if ENABLE_DEBUG_PRINT:
            print(f"Chunk #{chunk_id} streamingå“åº”ç»“æœ: {streaming_result[:MAX_DEBUG_OUTPUT_LENGTH]}...")
        
        # å°è¯•å¤šç§æ–¹å¼è§£æå“åº”
        result_json = None
        
        # æ–¹å¼1: ç›´æ¥è§£æä¸ºJSON
        try:
            result_json = json.loads(streaming_result)
            print(f"Chunk #{chunk_id} æˆåŠŸè§£æJSONå“åº”")
        except:
            print(f"Chunk #{chunk_id} ç›´æ¥JSONè§£æå¤±è´¥")
        
        # æ–¹å¼2: å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
        if not result_json:
            try:
                import re
                json_match = re.search(r'\{[\s\S]*\}', streaming_result)
                if json_match:
                    result_json = json.loads(json_match.group(0))
                    print(f"Chunk #{chunk_id} é€šè¿‡æ­£åˆ™æå–JSONæˆåŠŸ")
            except:
                print(f"Chunk #{chunk_id} æ­£åˆ™æå–JSONå¤±è´¥")
        
        # æ–¹å¼3: å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç®€å•çš„å­—ç¬¦ä¸²å“åº”
        if not result_json and streaming_result:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å“åº”ç»“æ„
            result_json = {
                "outputs": {
                    DIFY_OUTPUT_VARIABLE_NAME: streaming_result.strip()
                }
            }
            print(f"Chunk #{chunk_id} ä½¿ç”¨å­—ç¬¦ä¸²å“åº”æ¨¡å¼")
        
        if not result_json:
            raise ValueError(f"æ— æ³•ä»streamingå“åº”ä¸­è§£ææœ‰æ•ˆæ•°æ®: {streaming_result[:MAX_DEBUG_OUTPUT_LENGTH]}...")
        
        # æ‰“å°å®Œæ•´çš„å“åº”ç»“æ„ç”¨äºè°ƒè¯•
        if ENABLE_DEBUG_PRINT:
            print(f"Chunk #{chunk_id} è§£æåçš„å“åº”ç»“æ„: {json.dumps(result_json, ensure_ascii=False, indent=2)}")
        
        # --- ç¬¬ä¸‰æ­¥: è·å–å·¥ä½œæµç»“æœ ---
        # ä»streamingå“åº”ä¸­æå–æœ€ç»ˆç»“æœ
        if ENABLE_DEBUG_PRINT:
            print(f"Chunk #{chunk_id} å·¥ä½œæµè¿è¡Œå®Œæˆï¼Œæ­£åœ¨è§£æç»“æœ...")
            print(f"Chunk #{chunk_id} è§£æåçš„å“åº”ç»“æ„: {json.dumps(result_json, ensure_ascii=False, indent=2)}")
            print(f"Chunk #{chunk_id} èŠ‚ç‚¹ç±»å‹: {result_json.get('node_type')}")
            print(f"Chunk #{chunk_id} èŠ‚ç‚¹ID: {result_json.get('node_id')}")
            print(f"Chunk #{chunk_id} æ˜¯å¦æœ‰outputs: {'outputs' in result_json}")
            if 'outputs' in result_json:
                print(f"Chunk #{chunk_id} outputsé”®: {list(result_json['outputs'].keys())}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸèŠ‚ç‚¹çš„è¾“å‡º
        if result_json.get('node_type') == 'end' and 'outputs' in result_json:
            # è¿™æ˜¯ç»“æŸèŠ‚ç‚¹çš„è¾“å‡º
            outputs = result_json['outputs']
            if DIFY_OUTPUT_VARIABLE_NAME in outputs:
                download_url = outputs[DIFY_OUTPUT_VARIABLE_NAME]
                print(f"Chunk #{chunk_id} å·¥ä½œæµç»“æŸèŠ‚ç‚¹è¿”å›ä¸‹è½½é“¾æ¥: {download_url}")
                
                if download_url and isinstance(download_url, str) and download_url.startswith('http'):
                    if ENABLE_DEBUG_PRINT:
                        print(f"æ­£åœ¨ä¸º Chunk #{chunk_id} ä¸‹è½½ç»“æœæ–‡ä»¶...")
                    file_response = requests.get(download_url, timeout=FILE_DOWNLOAD_TIMEOUT)
                    file_response.raise_for_status()
                    
                    df_filtered_chunk = pd.read_excel(io.BytesIO(file_response.content))
                    
                    # å°Difyè¾“å‡ºçš„æ–‡ä»¶åº”è¯¥åŒ…å«idå’Œé¡¹ç›®åç§°åˆ—
                    if 'id' not in df_filtered_chunk.columns:
                        raise ValueError(f"ä¸‹è½½çš„ç»“æœæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°å…³é”®åˆ—: 'id'")
                        
                    filtered_ids = df_filtered_chunk['id'].tolist()
                    print(f"Chunk #{chunk_id} ä»ç»“æœæ–‡ä»¶ä¸­è§£æå‡º {len(filtered_ids)} ä¸ªID")
                    
                    return {'chunk_id': chunk_id, 'status': 'SUCCESS', 'download_url': download_url, 'data': filtered_ids}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥çš„è¾“å‡ºç»“æœï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
        if 'outputs' in result_json and DIFY_OUTPUT_VARIABLE_NAME in result_json['outputs']:
            # å¦‚æœå·¥ä½œæµç›´æ¥è¿”å›äº†è¾“å‡ºå˜é‡
            result_data = result_json['outputs'][DIFY_OUTPUT_VARIABLE_NAME]
            if ENABLE_DEBUG_PRINT:
                    print(f"Chunk #{chunk_id} å·¥ä½œæµè¾“å‡ºå˜é‡ '{DIFY_OUTPUT_VARIABLE_NAME}' çš„å€¼: {result_data}")
                    print(f"Chunk #{chunk_id} è¾“å‡ºå˜é‡ç±»å‹: {type(result_data)}")
            if isinstance(result_data, str) and result_data.startswith('http'):
                # å¦‚æœè¿”å›çš„æ˜¯ä¸‹è½½é“¾æ¥
                download_url = result_data
                print(f"Chunk #{chunk_id} å·¥ä½œæµè¿è¡ŒæˆåŠŸ, è·å¾—ä¸‹è½½é“¾æ¥: {download_url}")
                
                if ENABLE_DEBUG_PRINT:
                    print(f"æ­£åœ¨ä¸º Chunk #{chunk_id} ä¸‹è½½ç»“æœæ–‡ä»¶...")
                file_response = requests.get(download_url, timeout=FILE_DOWNLOAD_TIMEOUT)
                file_response.raise_for_status()
                
                df_filtered_chunk = pd.read_excel(io.BytesIO(file_response.content))
                
                if ID_COLUMN_NAME not in df_filtered_chunk.columns:
                    raise ValueError(f"ä¸‹è½½çš„ç»“æœæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°å…³é”®åˆ—: '{ID_COLUMN_NAME}'")
                    
                filtered_ids = df_filtered_chunk[ID_COLUMN_NAME].tolist()
                print(f"Chunk #{chunk_id} ä»ç»“æœæ–‡ä»¶ä¸­è§£æå‡º {len(filtered_ids)} ä¸ªID")
                
                # è¿”å›ä¸‹è½½é“¾æ¥è€Œä¸æ˜¯IDåˆ—è¡¨ï¼Œç”¨äºåç»­åˆå¹¶
                return {'chunk_id': chunk_id, 'status': 'SUCCESS', 'download_url': download_url, 'data': filtered_ids}
            elif isinstance(result_data, list):
                # å¦‚æœç›´æ¥è¿”å›äº†IDåˆ—è¡¨
                filtered_ids = result_data
                print(f"Chunk #{chunk_id} ä»å·¥ä½œæµè¾“å‡ºä¸­è·å¾— {len(filtered_ids)} ä¸ªID")
                return {'chunk_id': chunk_id, 'status': 'SUCCESS', 'data': filtered_ids}
            else:
                # å¦‚æœè¿”å›äº†å…¶ä»–æ ¼å¼ï¼Œå°è¯•è§£æ
                filtered_ids = []
                print(f"Chunk #{chunk_id} å·¥ä½œæµè¿”å›äº†éé¢„æœŸçš„æ•°æ®æ ¼å¼: {type(result_data)}")
                return {'chunk_id': chunk_id, 'status': 'SUCCESS', 'data': filtered_ids}
        else:
            # å¦‚æœæ²¡æœ‰ç›´æ¥çš„è¾“å‡ºï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä¸‹è½½é“¾æ¥
            download_url = result_json.get('outputs', {}).get(DIFY_OUTPUT_VARIABLE_NAME)
            
            if download_url and isinstance(download_url, str):
                print(f"Chunk #{chunk_id} å·¥ä½œæµè¿è¡ŒæˆåŠŸ, è·å¾—ä¸‹è½½é“¾æ¥: {download_url}")
                
                print(f"æ­£åœ¨ä¸º Chunk #{chunk_id} ä¸‹è½½ç»“æœæ–‡ä»¶...")
                file_response = requests.get(download_url, timeout=60)
                file_response.raise_for_status()
                
                df_filtered_chunk = pd.read_excel(io.BytesIO(file_response.content))
                
                if ID_COLUMN_NAME not in df_filtered_chunk.columns:
                    raise ValueError(f"ä¸‹è½½çš„ç»“æœæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°å…³é”®åˆ—: '{ID_COLUMN_NAME}'")
                    
                filtered_ids = df_filtered_chunk[ID_COLUMN_NAME].tolist()
                print(f"Chunk #{chunk_id} ä»ç»“æœæ–‡ä»¶ä¸­è§£æå‡º {len(filtered_ids)} ä¸ªID")
                
                # è¿”å›ä¸‹è½½é“¾æ¥è€Œä¸æ˜¯IDåˆ—è¡¨ï¼Œç”¨äºåç»­åˆå¹¶
                return {'chunk_id': chunk_id, 'status': 'SUCCESS', 'download_url': download_url, 'data': filtered_ids}
            else:
                # å¦‚æœæ—¢æ²¡æœ‰ç›´æ¥è¾“å‡ºä¹Ÿæ²¡æœ‰ä¸‹è½½é“¾æ¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
                filtered_ids = []
                print(f"Chunk #{chunk_id} å·¥ä½œæµæœªè¿”å›æœ‰æ•ˆçš„ç»“æœï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return {'chunk_id': chunk_id, 'status': 'SUCCESS', 'data': filtered_ids}

    except Exception as e:
        error_message = f"å¤„ç†Chunk #{chunk_id}æ—¶å‘ç”Ÿé”™è¯¯: {e}"
        # é¿å…å¤šæ¬¡è¯»å–å“åº”å†…å®¹ï¼Œåªè®°å½•åŸºæœ¬é”™è¯¯ä¿¡æ¯
        print(error_message)
        if ENABLE_TRACEBACK_PRINT:
            traceback.print_exc()
        return {'chunk_id': chunk_id, 'status': 'FAILED', 'error': error_message}


# 4. æ·»åŠ æ–‡ä»¶ä¸‹è½½è·¯ç”±
# ä»£ç†Dify APIçš„è·¯ç”±
@app.route('/v1/files/upload', methods=['POST'])
def proxy_dify_file_upload():
    """ä»£ç†Difyæ–‡ä»¶ä¸Šä¼ API"""
    try:
        print(f"=== æ–‡ä»¶ä¸Šä¼ ä»£ç†è¯·æ±‚ ===")
        print(f"è¯·æ±‚æ–¹æ³•: {request.method}")
        print(f"Content-Type: {request.content_type}")
        print(f"æ–‡ä»¶åˆ—è¡¨: {list(request.files.keys())}")
        print(f"è¡¨å•æ•°æ®: {dict(request.form)}")
        
        if 'file' not in request.files:
            return jsonify({"error": "æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶"}), 400
            
        uploaded_file = request.files['file']
        print(f"ä¸Šä¼ æ–‡ä»¶å: {uploaded_file.filename}")
        print(f"æ–‡ä»¶å¤§å°: {len(uploaded_file.read())} bytes")
        uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        print(f"æ–‡ä»¶MIMEç±»å‹: {uploaded_file.content_type}")
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if uploaded_file.filename:
            file_ext = uploaded_file.filename.rsplit('.', 1)[-1].lower()
            print(f"æ–‡ä»¶æ‰©å±•å: {file_ext}")
            if file_ext not in ALLOWED_EXTENSIONS:
                return jsonify({
                    "code": "unsupported_file_type",
                    "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}. æ”¯æŒçš„ç±»å‹: {', '.join(ALLOWED_EXTENSIONS)}",
                    "status": 415
                }), 415
        
        # å‡†å¤‡è½¬å‘åˆ°Dify APIçš„æ•°æ®
        files = {
            'file': (uploaded_file.filename, uploaded_file.stream, uploaded_file.content_type)
        }
        
        data = {}
        if 'user' in request.form:
            data['user'] = request.form['user']
            
        headers = {
            'Authorization': DIFY_API_KEY
        }
        
        print(f"è½¬å‘åˆ°Dify API: {DIFY_FILE_UPLOAD_URL}")
        print(f"è®¤è¯å¤´: {DIFY_API_KEY[:20]}...")
        
        response = requests.post(
            DIFY_FILE_UPLOAD_URL,
            files=files,
            data=data,
            headers=headers,
            timeout=REQUEST_TIMEOUT
        )
        
        print(f"Dify APIå“åº”çŠ¶æ€: {response.status_code}")
        print(f"Dify APIå“åº”å†…å®¹: {response.text}")
        
        if response.status_code == 200:
            return jsonify(response.json()), response.status_code
        else:
            # å¦‚æœDifyè¿”å›é”™è¯¯ï¼Œè¿”å›ç›¸åŒçš„é”™è¯¯
            try:
                error_data = response.json()
                return jsonify(error_data), response.status_code
            except:
                return jsonify({
                    "code": "dify_api_error",
                    "message": f"Dify APIé”™è¯¯: {response.text}",
                    "status": response.status_code
                }), response.status_code
        
    except Exception as e:
        print(f"ä»£ç†æ–‡ä»¶ä¸Šä¼ é”™è¯¯: {e}")
        if ENABLE_TRACEBACK_PRINT:
            traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/v1/workflows/run', methods=['POST'])
def proxy_dify_workflow():
    """ä»£ç†Difyå·¥ä½œæµAPI"""
    try:
        request_data = request.get_json()
        print(f"=== å·¥ä½œæµAPIè°ƒç”¨ ===")
        print(f"è¯·æ±‚æ•°æ®: {json.dumps(request_data, indent=2, ensure_ascii=False)}")
        
        # è½¬å‘è¯·æ±‚åˆ°çœŸå®çš„Dify API
        headers = {
            'Authorization': DIFY_API_KEY,
            'Content-Type': 'application/json'
        }
        
        print(f"è½¬å‘åˆ°Dify API: {DIFY_WORKFLOW_RUN_URL}")
        print(f"è®¤è¯å¤´: {DIFY_API_KEY[:20]}...")
        
        response = requests.post(
            DIFY_WORKFLOW_RUN_URL,
            json=request_data,
            headers=headers,
            timeout=REQUEST_TIMEOUT
        )
        
        print(f"Dify APIå“åº”çŠ¶æ€: {response.status_code}")
        print(f"Dify APIå“åº”å†…å®¹: {response.text}")
        
        # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºJSONæ ¼å¼
        try:
            response_json = response.json()
            return jsonify(response_json), response.status_code
        except ValueError as json_error:
            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
            print(f"å“åº”ä¸æ˜¯JSONæ ¼å¼: {json_error}")
            return response.text, response.status_code, {'Content-Type': response.headers.get('Content-Type', 'text/plain')}
        
    except requests.exceptions.RequestException as req_error:
        print(f"è¯·æ±‚é”™è¯¯: {req_error}")
        return jsonify({"error": f"è¯·æ±‚Dify APIå¤±è´¥: {str(req_error)}"}), 500
    except Exception as e:
        print(f"ä»£ç†å·¥ä½œæµé”™è¯¯: {e}")
        if ENABLE_TRACEBACK_PRINT:
            traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/downloads/<filename>')
def download_file(filename):
    """
    ä»DOWNLOAD_FOLDERç›®å½•ä¸­æä¾›é™æ€æ–‡ä»¶ä¸‹è½½ã€‚
    """
    return send_from_directory(DOWNLOAD_FOLDER, filename, as_attachment=True)

# 5. ä¸»APIç«¯ç‚¹ (æ— å˜åŒ–)
@app.route('/process-large-excel', methods=['POST'])
def process_large_excel():
    print(f"Request Method: {request.method}")
    print(f"Request URL: {request.url}")
    print(f"Request Headers: {request.headers}")
    print(f"Request Content-Type: {request.content_type}")
    
    # å°è¯•è·å–åŸå§‹è¯·æ±‚æ•°æ®ï¼Œæ— è®ºContent-Typeæ˜¯ä»€ä¹ˆ
    try:
        raw_data = request.get_data()
        print(f"Raw Request Data (first 500 bytes): {raw_data[:500].decode('utf-8', errors='ignore')}")
    except Exception as e:
        print(f"Error getting raw request data: {e}")

    # å°è¯•è§£æJSONæ•°æ®ï¼Œå¦‚æœContent-Typeæ˜¯application/json
    if request.is_json:
        try:
            json_data = request.get_json(silent=True)
            print(f"Parsed JSON Data: {json.dumps(json_data, ensure_ascii=False, indent=2)}")
        except Exception as e:
            print(f"Error parsing JSON data: {e}")
    else:
        print("Request is not JSON.")
    if 'file' not in request.files: return jsonify({"error": "è¯·æ±‚ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶éƒ¨åˆ†"}), 400
    file = request.files['file']
    if file.filename == '': return jsonify({"error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400
    
    # è·å–which_aspectså‚æ•°
    which_aspects = request.form.get('which_aspects', 'æ°´è´¨ã€æ°´åŠ¡ã€æ°´åˆ©çš„æ‹›æ ‡ä¿¡æ¯æ•°æ®')  # é»˜è®¤å€¼
    print(f"æ¥æ”¶åˆ°çš„which_aspectså‚æ•°: {which_aspects}")
    
    if file:
        large_excel_path = os.path.join(UPLOAD_FOLDER, f"large_{uuid.uuid4().hex[:UUID_LENGTH]}.xlsx")
        file.save(large_excel_path)
        start_time = time.time()

        try:
            df_large = pd.read_excel(large_excel_path)
            # çœŸæ­£æ–°å¢ä¸€åˆ—è‡ªå¢ idï¼Œè€Œä¸æ˜¯ç”¨æ—§ç´¢å¼•
            df_large.insert(0, ID_COLUMN_NAME, range(len(df_large)))
            print(f"æˆåŠŸåŠ è½½Excelå¹¶è‡ªåŠ¨æ·»åŠ äº† '{ID_COLUMN_NAME}' åˆ—ã€‚æ€»è¡Œæ•°: {len(df_large)}")
            chunk_size = DEFAULT_CHUNK_SIZE  # ä»é…ç½®è¯»å–
            print(f"ä½¿ç”¨chunkå¤§å°: {chunk_size} è¡Œ")
            # åˆ›å»ºchunkåˆ—è¡¨ - åˆ‡åˆ†åè‡ªè¡Œå¹¶å‘è°ƒç”¨ï¼Œæ— éœ€æŒ‰é¡ºåº
            df_chunks = [(i // chunk_size, df_large.iloc[i:i + chunk_size]) for i in range(0, len(df_large), chunk_size)]
            total_chunks = len(df_chunks)
            
            # å­˜å‚¨æ‰€æœ‰ç»“æœçš„çº¿ç¨‹å®‰å…¨å®¹å™¨
            import threading
            results_lock = threading.Lock()
            all_filtered_ids = []
            download_urls = []
            final_results_df = pd.DataFrame()
            final_results_json = []
            retry_count = 0
            
            # è·Ÿè¸ªchunkå¤„ç†çŠ¶æ€
            chunk_status = {}
            for chunk_id, _ in df_chunks:
                chunk_status[chunk_id] = {'status': 'pending', 'retries': 0}

            def process_chunk_concurrent(chunk_id, chunk_df, which_aspects_value):
                """å¹¶å‘å¤„ç†å•ä¸ªchunkï¼Œæ”¯æŒæ— é™é‡è¯•ç›´åˆ°æˆåŠŸ"""
                retry = 0
                while True:  # æ— é™å¾ªç¯ç›´åˆ°æˆåŠŸ
                    try:
                        result = call_small_workflow(chunk_id, chunk_df, which_aspects_value)
                        if result['status'] == 'SUCCESS':
                            # çº¿ç¨‹å®‰å…¨åœ°å¤„ç†ç»“æœ
                            with results_lock:
                                chunk_status[chunk_id]['status'] = 'success'  # æ ‡è®°ä¸ºæˆåŠŸçŠ¶æ€
                                if result.get('download_url'):
                                    download_urls.append(result['download_url'])
                                    # å®æ—¶å¤„ç†ç»“æœæ•°æ®
                                    try:
                                        file_response = requests.get(result['download_url'], timeout=FILE_DOWNLOAD_TIMEOUT)
                                        file_response.raise_for_status()
                                        df_chunk_result = pd.read_excel(io.BytesIO(file_response.content))
                                        
                                        if 'id' in df_chunk_result.columns:
                                            chunk_ids = df_chunk_result['id'].tolist()
                                            
                                            # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„IDï¼Œé¿å…é‡å¤
                                            new_chunk_ids = [cid for cid in chunk_ids if cid not in all_filtered_ids]
                                            all_filtered_ids.extend(new_chunk_ids)
                                            
                                            if new_chunk_ids:  # åªå¤„ç†æ–°çš„ID
                                                # ä½¿ç”¨å° Dify è¿”å›çš„ id ä» df_large ä¸­æå–å®Œæ•´è¡Œ
                                                matched_rows = df_large[df_large[ID_COLUMN_NAME].isin(new_chunk_ids)].copy()
                                                
                                                # ç§»é™¤ ID åˆ—
                                                matched_rows = matched_rows.drop(columns=[ID_COLUMN_NAME], errors='ignore')
                                                
                                                # å‡è®¾å…³é”®è¯åˆ—åä¸º 'å…³é”®è¯'ï¼Œå°†å…¶ç§»åˆ°ç¬¬ä¸€åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                                if 'å…³é”®è¯' in matched_rows.columns:
                                                    columns = ['å…³é”®è¯'] + [col for col in matched_rows.columns if col != 'å…³é”®è¯']
                                                    matched_rows = matched_rows[columns]
                                                
                                                # åˆå¹¶åˆ°æœ€ç»ˆ DataFrame
                                                final_results_df = pd.concat([final_results_df, matched_rows], ignore_index=True)
                                                
                                                # å°†æ¯è¡Œæ•°æ®è½¬æ¢ä¸ºå­—å…¸å¹¶æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                                                for _, row in matched_rows.iterrows():
                                                    row_dict = row.to_dict()
                                                    # åˆ é™¤å¯èƒ½çš„å‰©ä½™ ID åˆ—
                                                    for col in COLUMNS_TO_REMOVE:
                                                        if col in row_dict:
                                                            del row_dict[col]
                                                    final_results_json.append(row_dict)
                                    except Exception:
                                        pass
                            return {'status': 'SUCCESS', 'chunk_id': chunk_id, 'download_url': result.get('download_url', '')}
                        else:
                            # å¤±è´¥é‡è¯•
                            retry += 1
                            with results_lock:
                                chunk_status[chunk_id]['retries'] += 1
                            print(f"Chunk #{chunk_id} ç¬¬{retry}æ¬¡é‡è¯•...")
                            time.sleep(RETRY_DELAY)
                    except Exception as e:
                        # å¼‚å¸¸é‡è¯•
                        retry += 1
                        with results_lock:
                            chunk_status[chunk_id]['retries'] += 1
                        print(f"Chunk #{chunk_id} ç¬¬{retry}æ¬¡é‡è¯•ï¼Œå¼‚å¸¸: {str(e)[:100]}...")
                        time.sleep(RETRY_DELAY)

            # å¹¶å‘å¤„ç†æ‰€æœ‰chunk - å®Œå…¨éšæœºå¹¶å‘ï¼Œä¸é™åˆ¶é¡ºåº
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                # ä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œè®©çº¿ç¨‹æ± è‡ªç”±è°ƒåº¦
                future_to_chunk = {
                    executor.submit(process_chunk_concurrent, chunk_id, chunk_df, which_aspects): chunk_id 
                    for chunk_id, chunk_df in df_chunks
                }
                
                # å®æ—¶å¤„ç†å®Œæˆçš„ä»»åŠ¡ï¼ˆæ— éœ€ç­‰å¾…æ‰¹æ¬¡ï¼‰
                for future in as_completed(future_to_chunk):
                    result = future.result()
                    if result and result.get('status') == 'FAILED':
                        chunk_id = result.get('chunk_id', 'æœªçŸ¥')
                        print(f"Chunk {chunk_id} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    elif result and result.get('status') == 'SUCCESS':
                        chunk_id = result.get('chunk_id', 'æœªçŸ¥')
                        print(f"Chunk {chunk_id} å¤„ç†æˆåŠŸ")
                        
            # æ˜¾ç¤ºæœ€ç»ˆå¤„ç†ç»Ÿè®¡
            successful_chunks = len([cid for cid, status in chunk_status.items() if status['status'] == 'success'])
            total_retries = sum([status['retries'] for status in chunk_status.values()])
            print(f"å¤„ç†å®Œæˆ - æ€»chunkæ•°: {total_chunks}, æˆåŠŸ: {successful_chunks}")
            
            # å¦‚æœæ²¡æœ‰å®æ—¶æ±‡æ€»æ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨å¤„ç†æ–¹å¼
            if len(final_results_json) == 0:
                print("ä½¿ç”¨å¤‡ç”¨å¤„ç†æ–¹å¼ï¼šä¸‹è½½æ‰€æœ‰chunkç»“æœæ–‡ä»¶å¹¶é‡æ–°å¤„ç†")
                
                # ç”¨äºå»é‡çš„IDé›†åˆ
                processed_ids = set()
                
                # ä¸‹è½½æ‰€æœ‰æˆåŠŸçš„chunkç»“æœæ–‡ä»¶
                for download_url in download_urls:
                    try:
                        # ä¸‹è½½æ–‡ä»¶
                        file_response = requests.get(download_url, timeout=FILE_DOWNLOAD_TIMEOUT)
                        file_response.raise_for_status()
                        
                        # è¯»å–Excelæ•°æ®
                        df_chunk_result = pd.read_excel(io.BytesIO(file_response.content))
                        
                        if not df_chunk_result.empty and 'id' in df_chunk_result.columns:
                            # è·å–å°Difyè¿”å›çš„IDåˆ—è¡¨
                            chunk_ids = df_chunk_result['id'].tolist()
                            
                            # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„IDï¼Œé¿å…é‡å¤
                            new_chunk_ids = [cid for cid in chunk_ids if cid not in processed_ids]
                            processed_ids.update(new_chunk_ids)
                            
                            if new_chunk_ids:  # åªå¤„ç†æ–°çš„ID
                                # ä½¿ç”¨IDä»åŸå§‹æ•°æ®ä¸­æå–å®Œæ•´è¡Œï¼ˆä¿æŒåŸå§‹æ•°æ®å®Œæ•´æ€§ï¼‰
                                matched_rows = df_large[df_large[ID_COLUMN_NAME].isin(new_chunk_ids)].copy()
                                
                                # ç§»é™¤IDåˆ—
                                matched_rows = matched_rows.drop(columns=[ID_COLUMN_NAME], errors='ignore')
                                
                                # å°†å…³é”®è¯åˆ—ç§»åˆ°ç¬¬ä¸€åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                if 'å…³é”®è¯' in matched_rows.columns:
                                    columns = ['å…³é”®è¯'] + [col for col in matched_rows.columns if col != 'å…³é”®è¯']
                                    matched_rows = matched_rows[columns]
                                
                                # åˆå¹¶åˆ°æœ€ç»ˆDataFrame
                                final_results_df = pd.concat([final_results_df, matched_rows], ignore_index=True)
                                
                                # è½¬æ¢ä¸ºJSONæ ¼å¼
                                for _, row in matched_rows.iterrows():
                                    row_dict = row.to_dict()
                                    # åˆ é™¤å¯èƒ½çš„å‰©ä½™IDåˆ—
                                    for col in COLUMNS_TO_REMOVE:
                                        if col in row_dict:
                                            del row_dict[col]
                                    final_results_json.append(row_dict)
                                
                    except Exception as e:
                        print(f"å¤„ç†ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
                        continue
                    

            
            # åœ¨ä¿å­˜æœ€ç»ˆç»“æœæ–‡ä»¶ä¹‹å‰è¿›è¡Œæ’åºå’Œå»é‡
            if not final_results_df.empty:
                # å…ˆå»é‡ - åŸºäºæ‰€æœ‰åˆ—çš„ç»„åˆå»é‡
                print(f"å»é‡å‰è®°å½•æ•°: {len(final_results_df)}")
                final_results_df = final_results_df.drop_duplicates()
                print(f"å»é‡åè®°å½•æ•°: {len(final_results_df)}")
                
                # ç¡®ä¿å…³é”®è¯å’Œæ—¶é—´åˆ—å­˜åœ¨
                if 'å…³é”®è¯' in final_results_df.columns and 'æ—¶é—´' in final_results_df.columns:
                    print("æ­£åœ¨å¯¹æœ€ç»ˆç»“æœè¿›è¡Œæ’åºï¼šå…ˆæŒ‰å…³é”®è¯ï¼Œå†æŒ‰æ—¶é—´...")
                    # å…ˆæŒ‰å…³é”®è¯æ’åºï¼ŒåŒç±»åˆ«å†…å†æŒ‰æ—¶é—´æ’åº
                    final_results_df = final_results_df.sort_values(['å…³é”®è¯', 'æ—¶é—´'], ascending=[True, True])
                    print(f"æ’åºå®Œæˆï¼Œå…± {len(final_results_df)} æ¡è®°å½•")
                else:
                    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°å…³é”®è¯æˆ–æ—¶é—´åˆ—ï¼Œè·³è¿‡æ’åº")
                    
                # é‡æ–°æ„å»ºfinal_results_jsonä»¥ç¡®ä¿ä¸DataFrameä¸€è‡´
                final_results_json = []
                for _, row in final_results_df.iterrows():
                    row_dict = row.to_dict()
                    # åˆ é™¤å¯èƒ½çš„å‰©ä½™IDåˆ—
                    for col in COLUMNS_TO_REMOVE:
                        if col in row_dict:
                            del row_dict[col]
                    final_results_json.append(row_dict)
            
            # ä¿å­˜æœ€ç»ˆç»“æœæ–‡ä»¶
            final_filename = f"final_result_{uuid.uuid4().hex[:UUID_LENGTH]}.xlsx"
            final_filepath = os.path.join(DOWNLOAD_FOLDER, final_filename)
            
            # ä½¿ç”¨æœ€ç»ˆå¤„ç†çš„æ•°æ®
            final_df_to_save = final_results_df
            
            # ä¿å­˜æ–‡ä»¶ - ç¡®ä¿ç§»é™¤æ‰€æœ‰å¯èƒ½çš„IDåˆ—å’Œç´¢å¼•åˆ—
            # æ£€æŸ¥å¹¶åˆ é™¤æ‰€æœ‰å¯èƒ½çš„IDåˆ—
            for col in COLUMNS_TO_REMOVE:
                if col in final_df_to_save.columns:
                    final_df_to_save = final_df_to_save.drop(columns=[col])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å­—ç´¢å¼•åˆ—ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€åˆ—ï¼‰
            if len(final_df_to_save.columns) > 0:
                first_col = final_df_to_save.columns[0]
                # å¦‚æœç¬¬ä¸€åˆ—æ˜¯æ•°å­—ä¸”ä¸æ˜¯é¢„æœŸçš„å…³é”®è¯åˆ—ï¼Œåˆ™åˆ é™¤å®ƒ
                if first_col.isdigit() or first_col in ['index', 'Unnamed: 0']:
                    final_df_to_save = final_df_to_save.drop(columns=[first_col])
            
            # ç¡®ä¿ä¸ä¿å­˜ç´¢å¼•ä½œä¸ºåˆ—
            final_df_to_save.to_excel(final_filepath, index=False)
            
            # ä¸Šä¼ æ–‡ä»¶åˆ°æ–‡ä»¶æœåŠ¡å™¨
            try:
                with open(final_filepath, 'rb') as f:
                    files = {'file': (final_filename, f, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')}
                    upload_response = requests.post(f"http://{FILE_SERVER_HOST}:{FILE_SERVER_PORT}/upload", files=files)
                    upload_response.raise_for_status()
                    final_download_url = upload_response.json().get('download_url', '')
            except Exception as e:
                final_download_url = f"http://{FILE_SERVER_HOST}:{FILE_SERVER_PORT}/downloads/{final_filename}"
            
            end_time = time.time()
            
            # æ„å»ºè¿”å›ç»“æœ
            successful_chunks = len([cid for cid, status in chunk_status.items() if status['status'] == 'success'])
            response_data = {
                "message": "å¤„ç†å®Œæˆ",
                "summary": { 
                    "total_chunks": total_chunks, 
                    "successful_chunks": successful_chunks,
                    "chunk_size": chunk_size,
                    "retry_mode": "infinite_retries"  # æ ‡è¯†ä½¿ç”¨æ— é™é‡è¯•æ¨¡å¼
                },
                "processing_time": f"{end_time - start_time:.2f} ç§’",
                "total_filtered_count": len(final_results_json),
                "filtered_data": final_results_json
            }
            
            # å¦‚æœæœ‰æœ€ç»ˆä¸‹è½½é“¾æ¥ï¼Œæ·»åŠ åˆ°å“åº”ä¸­
            if final_download_url:
                response_data["final_download_url"] = final_download_url
            
            return jsonify(response_data)

        except Exception as e:
            if ENABLE_TRACEBACK_PRINT:
                traceback.print_exc()
            return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯", "details": str(e), "trace": traceback.format_exc() if ENABLE_TRACEBACK_PRINT else None}), 500
        finally:
            if os.path.exists(large_excel_path): os.remove(large_excel_path)


    return jsonify({"error": "æ–‡ä»¶å¤„ç†å¤±è´¥"}), 500


# 5. å¯åŠ¨WebæœåŠ¡
if __name__ == '__main__':
    app.run(host=FLASK_HOST, port=FLASK_PORT, debug=FLASK_DEBUG)